{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. \n",
        "Licensed under the MIT license. \n",
        "## Model Training Script for Synapse-AI-Retail-Recommender  \n",
        "Model Author (Data Scientist): Xiaoyong Zhu  \n",
        "  \n",
        "This script is an adapted script of the full Model Training script that can be found in `4. ML Model Building`. This is a slimmed down version that only has the required operations for producing a model that the Model Deployment Process and the RecommendationRefresh notebook can consume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from pyspark.sql.functions import unix_timestamp\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.feature import RFormula\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.table(\"retailaidb.cleaned_dataset\")\n",
        "spark.sparkContext.setCheckpointDir('checkpoint/')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Filter only for Electronics items\n",
        "\n",
        "df = df.withColumn('category_code_new', df['category_code'].substr(0, 11))\n",
        "df = df.filter(\"category_code_new = 'electronics'\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "top_category = df.groupBy('category_code_new').count().sort('count', ascending=False).limit(5) # only keep top 5 categories\n",
        "top_category = top_category.withColumnRenamed(\"category_code\",\"category_code_tmp\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "item_to_save = df.groupBy('product_id', \"category_code\").count().sort('count', ascending=False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "item_to_save = item_to_save.join(top_category, top_category.category_code_tmp == item_to_save.category_code).limit(20)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "raw_df = df\n",
        "\n",
        "product_count = df.groupBy('product_id').count()\n",
        "product_count = product_count.filter(\"count >= 30000\").orderBy('count', ascending=False) #only counts when the product has 20000 views\n",
        "\n",
        "raw_df = raw_df.withColumnRenamed(\"product_id\",\"product_id_tmp\")\n",
        "raw_df = raw_df.join(product_count, raw_df.product_id_tmp == product_count.product_id)\n",
        "\n",
        "user_count = df.groupBy('user_id').count()\n",
        "user_count = user_count.filter(\"count >= 200\").orderBy('count', ascending=False) #only counts when the user has 100 clicks\n",
        "\n",
        "raw_df = raw_df.withColumnRenamed(\"user_id\",\"user_id_tmp\")\n",
        "raw_df = raw_df.join(user_count, raw_df.user_id_tmp == user_count.user_id)\n",
        "\n",
        "df = raw_df\n",
        "\n",
        "df = df.where(df.event_type == \"view\")\n",
        "df = df.drop(\"event_time\",\"category_code\",\"user_session\",\"price\",\"brand\",\"category_id\")\n",
        "df = df.groupBy([df.product_id, df.user_id]).count()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# save table for further use\n",
        "df.write.saveAsTable(\"retailaidb.cleaned_dataset_electronics\", mode=\"overwrite\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#import the required functions and libraries\n",
        "from pyspark.sql.functions import *"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "df = df.withColumn(\"user_id\", df[\"user_id\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"product_id\", df[\"product_id\"].cast(IntegerType()))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#split the data into training and test datatset\n",
        "train,test=df.randomSplit([0.75,0.25])"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#import ALS recommender function from pyspark ml library\n",
        "from pyspark.ml.recommendation import ALS\n",
        "#Training the recommender model using train datatset\n",
        "rec=ALS(maxIter=40,regParam=0.20,implicitPrefs = True, userCol='user_id',itemCol='product_id',ratingCol='count',nonnegative=True,coldStartStrategy=\"drop\", rank=25)\n",
        "#fit the model on train set\n",
        "rec_model=rec.fit(train)\n",
        "#making predictions on test set \n",
        "predicted_ratings=rec_model.transform(test)\n",
        "#columns in predicted ratings dataframe\n",
        "predicted_ratings.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "predicted_ratings_witherr=predicted_ratings.withColumn('err',abs(predicted_ratings[\"prediction\"] - predicted_ratings[\"count\"]))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#importing Regression Evaluator to measure RMSE\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#create Regressor evaluator object for measuring accuracy\n",
        "evaluator=RegressionEvaluator(metricName='rmse',predictionCol='prediction',labelCol='count')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#apply the RE on predictions dataframe to calculate RMSE\n",
        "rmse=evaluator.evaluate(predicted_ratings)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Save the model\n",
        "rec_model.write().overwrite().save(\"retailai_recommendation_model\")"
      ],
      "attachments": {}
    }
  ],
  "metadata": {
    "description": "xiaoyong's test notebook",
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}